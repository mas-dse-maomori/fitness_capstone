{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create pyspark dataframe from csv\n",
    "def df_from_csv(csv_file):\n",
    "    text = sc.textFile(csv_file)\\\n",
    "        .map(lambda line: line.split(\",\"))\n",
    "    #didn't work with take(1). believe returns \n",
    "    #different object then first()\n",
    "    schema = text.first()\n",
    "    data = text.filter(lambda x: x != schema)\n",
    "    df = sqlContext.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "def change_column_names(df, old_names, new_names):\n",
    "    pass\n",
    "    return df;\n",
    "\n",
    "##input\n",
    "##   string: 2013-02-22 21:38:45\n",
    "##output: \n",
    "##   list: [u'2013', u'02', u'22', u'21', u'38', u'45']\n",
    "def datetime_to_trace_time(timestamp):\n",
    "    #strip double quotes\n",
    "    timestamp = timestamp.replace('\"', '')\n",
    "    r_list = re.split('[- :]', timestamp)\n",
    "    return [int(x) for x in r_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|              \"time\"|\"altitude\"|\"heart_rate\"|   \"latitude\"|  \"longitude\"|     \"speed\"|\"workoutid\"|    \"id\"|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|\"2013-02-22 21:38...|          |   147.00000|51.4394768234|-0.8953504544|6.7140000000|  167479013|13856798|\n",
      "|\"2013-02-22 21:38...|          |   148.00000|51.4392022323|-0.8952703234|9.3492000000|  167479013|13856799|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_run='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_data_with_hr_spd.csv'\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "df_runs = df_from_csv(data_run)\n",
    "print type(df_runs)\n",
    "df_runs.show(2)\n",
    "df_runs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = df_runs.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import nbimporter\n",
    "#for ordered dict for traces implementation\n",
    "from collections import OrderedDict\n",
    "\n",
    "import traces\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "#for timedelta manipulation\n",
    "from math import fabs\n",
    "\n",
    "#kmeans\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from  pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc =SparkContext()\n",
    "\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#%load_ext autotime\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#del min\n",
    "#del max\n",
    "\n",
    "data_run='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_data_with_hr_spd.csv'\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get list of workoutids\n",
    "#woutid_list = df_runs.select('\"workoutid\"').distinct()\\\n",
    "#    .rdd.map(lambda r:r[0]).collect()\n",
    "#woutid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class to create summary stats of workouts\n",
    "class SummaryStat():\n",
    "    data_points = ''\n",
    "    new_col_names = []\n",
    "    #list of tuples. \n",
    "    #   key: column name\n",
    "    #   value: metric (sum/avg/etc)\n",
    "    summary_conf = ''\n",
    "    partition_name = ''\n",
    "    def __init__(self, sql_df, summary_conf = [], \\\n",
    "                partition_name = ''):\n",
    "        if partition_name:\n",
    "            self.partition_name = partition_name\n",
    "        else:\n",
    "            self.partition_name = '\"workoutid\"'\n",
    "        self.data_points = sql_df\n",
    "        if not isinstance(sql_df, DataFrame):\n",
    "            print 'please enter spark dataframe object'\n",
    "            return\n",
    "        #if summary_conf not defined, default to sum for all columns\n",
    "        if not summary_conf:\n",
    "            col_names = self.data_points.schema.names\n",
    "            for i in col_names:\n",
    "                summary_conf.append((i, 'sum'))\n",
    "        else:\n",
    "            if not isinstance(summary_conf, list):\n",
    "                print 'check summary_conf type'\n",
    "                return\n",
    "            for element in summary_conf:\n",
    "                if not isinstance(element, dict):\n",
    "                    print 'check summary_conf type 2'\n",
    "                    return\n",
    "            self.summary_conf = summary_conf\n",
    "        \n",
    "    def describe(self):\n",
    "        print self.data_points.limit(2)\n",
    "\n",
    "    def get_new_col_names(self):\n",
    "        return self.new_col_names\n",
    "        \n",
    "    def generate_summary(self):\n",
    "        #rewnew new_col_names\n",
    "        self.new_col_names = []\n",
    "        #windows function test\n",
    "        r_df = ''\n",
    "        r_list_cols = []\n",
    "        for dic in self.summary_conf:\n",
    "            col_name = dic.keys()[0] #should only have 1 key per dictionary\n",
    "            aggregation_type = dic[col_name]\n",
    "            w = Window.partitionBy(self.partition_name)\n",
    "            if aggregation_type == 'avg':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        avg(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())\n",
    "                self.new_col_names.append(new_col_name)\n",
    "\n",
    "            if aggregation_type == 'sum':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        sum(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())   \n",
    "                self.new_col_names.append(new_col_name)\n",
    "\n",
    "                \n",
    "            if aggregation_type == 'max':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        max(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())    \n",
    "                self.new_col_names.append(new_col_name)\n",
    "                \n",
    "            if aggregation_type == 'min':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        min(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct()) \n",
    "                self.new_col_names.append(new_col_name)\n",
    "                \n",
    "        for df in r_list_cols:\n",
    "            if not r_df:\n",
    "                r_df = df\n",
    "            else:\n",
    "                r_df = r_df.join(df, (r_df[self.partition_name]==df[self.partition_name]), 'inner')\\\n",
    "                    .drop(df[self.partition_name])\n",
    "        return r_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#minMaxScaler wrapper since originalMin/Max is only implemented in 2.0\n",
    "class scaler_wrapper():\n",
    "    mmModel = ''\n",
    "    originalMin = ''\n",
    "    originalMax = ''   \n",
    "    \n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        self.mmModel = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
    "        self.in_column = inputCol\n",
    "        \n",
    "    def get_input_col_name(self):\n",
    "        return self.mmModel.getInputCol()\n",
    "\n",
    "    def getMax(self):\n",
    "        return self.mmModel.getMax()\n",
    "        \n",
    "    def getMin(self):\n",
    "        return self.mmModel.getMin()\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'describe'\n",
    "    \n",
    "    def fit(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        self.originalMin = df.select(col).rdd.flatMap(lambda x: x[0]).min()\n",
    "        self.originalMax = df.select(col).rdd.flatMap(lambda x: x[0]).max()\n",
    "        return self.mmModel.fit(df)\n",
    "    \n",
    "    #denormalize the value\n",
    "    def denormalize(self, value):\n",
    "        v = (value-self.getMin())*\\\n",
    "            (self.originalMax - self.originalMin)*\\\n",
    "            (self.getMax()-self.getMin()) + self.originalMin\n",
    "        if v or v == 0:\n",
    "            return v\n",
    "        else:\n",
    "            return -999\n",
    "        \n",
    "    def normalize(self, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#change column type\n",
    "def change_type_cols_double(df, list_cols):\n",
    "    for col_name in list_cols:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "#vectorize the column\n",
    "#keeps the original name\n",
    "def vectorize_columns(df, list_cols):\n",
    "    tmp_col_name = 'temp'\n",
    "    for col_name in list_cols:\n",
    "        vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "        df = df.withColumn(tmp_col_name, vectorize(df[col_name])).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "    return df\n",
    "\n",
    "##scaler wrapper usage\n",
    "#generate normalized dataframe\n",
    "#keep the original column names\n",
    "#returns:\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df(df, list_cols):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "# MinMaxScaler doesn't have originalMin(only supported in 2.0). made one above cell with wrapper class\n",
    "# #generate normalized dataframe\n",
    "# #keep the original column names\n",
    "# #returns:\n",
    "# #   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "# def normalize_df(df, list_cols):\n",
    "#     tmp_col_name = 'temp'\n",
    "#     r_dict = {}\n",
    "#     for col_name in list_cols:\n",
    "#         scaler = MinMaxScaler(inputCol=col_name, outputCol=tmp_col_name)\n",
    "#         scalerModel = scaler.fit(df)\n",
    "#         df = scalerModel.transform(df).drop(col_name)\\\n",
    "#             .withColumnRenamed(tmp_col_name, col_name)\n",
    "#         r_dict[col_name] = [scaler, scalerModel]\n",
    "#     return df, r_dict\n",
    "\n",
    "#input: (MLLIB KMEANS)\n",
    "#   cols_to_cluster: list of column names to cluster\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "#     vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "#         outputCol='features')\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "#     df_to_cluster = df_to_cluster.select('features')\n",
    "#     df_to_cluster = df_to_cluster.rdd\\\n",
    "#         .map(lambda row : Vectors.dense([item for item in row]))\n",
    "#     clusters = KMeans.train(df_to_cluster, num_clusters,\\\n",
    "#                            maxIterations = 10,\\\n",
    "#                         initializationMode=\"random\")\n",
    "    \n",
    "#     return clusters\n",
    "\n",
    "\n",
    "#ML KMEANS\n",
    "#returns KMeanModel, df_to_cluster\n",
    "def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "    vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "        outputCol='features')\n",
    "    df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "    df_to_cluster = df_to_cluster.select('features')\n",
    "    #df_to_cluster = df_to_cluster.rdd\\\n",
    "     #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "    kmeans = KMeans(k=num_clusters, seed=1)\n",
    "    model = kmeans.fit(df_to_cluster)\n",
    "    return model, df_to_cluster\n",
    "    \n",
    "#returns clusters. show cluster centerse by clusters.clusterCenters.\n",
    "def cluster_df(df, cols_to_normalize, num_clusters = 2, return_sse = False, cols_to_cluster = []):\n",
    "    if not cols_to_cluster:\n",
    "        cols_to_cluster = cols_to_normalize\n",
    "    df = change_type_cols_double(df, cols_to_normalize)\n",
    "    df = vectorize_columns(df, cols_to_normalize)\n",
    "    df, scalerModels = normalize_df(df, cols_to_cluster)\n",
    "    r_obj = cluster_summary_df(df, cols_to_cluster, num_clusters)\n",
    "    \n",
    "    if return_sse:\n",
    "        #calculate SSE for this kmean model\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #r_obj = [KMeanModel, new_df]\n",
    "    return r_obj[0], r_obj[1], scalerModels\n",
    "\n",
    "def denormalize_centers(list_centers, scaler_models):\n",
    "    new_centers = []\n",
    "    for center in list_centers:\n",
    "        n_center = []\n",
    "        for c in range(len(center)):\n",
    "            n_center.append(scaler_models[c].denormalize(center[c]))\n",
    "        new_centers.append(n_center)\n",
    "    return new_centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n",
      "+------------------+-----------+-----------+--------------+\n",
      "|  avg_\"heart_rate\"|avg_\"speed\"|\"workoutid\"|avg_\"altitude\"|\n",
      "+------------------+-----------+-----------+--------------+\n",
      "|             103.0|        0.0|  165504204|          null|\n",
      "|150.06666666666666|   10.59552|  167479013|          null|\n",
      "+------------------+-----------+-----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define columns to cluster on (they will be normalized)\n",
    "#cols_to_cluster = ['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n",
    "#create summary stats for workouts\n",
    "\n",
    "s_conf = [{'\"heart_rate\"': 'avg'}, {'\"speed\"': 'avg'}, {'\"altitude\"': 'avg'}]\n",
    "\n",
    "#obj = SummaryStat(df_runs, summary_conf=s_conf)\n",
    "obj = SummaryStat(sub_df, summary_conf=s_conf)\n",
    "\n",
    "#obj.describe()\n",
    "sumstats = obj.generate_summary()\n",
    "cols_to_cluster = obj.get_new_col_names()\n",
    "print cols_to_cluster\n",
    "sumstats.show()\n",
    "sumstats.count()\n",
    "#clusters = cluster_df(sumstats, cols_to_cluster, 5)\n",
    "#clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_obj = cluster_df(sumstats, cols_to_cluster, num_clusters=2) \n",
    "kmodel = r_obj[0]\n",
    "new_df = r_obj[1]\n",
    "scaler_model_dict = r_obj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.0, 0.0, 0.0], [150.06666666666666, 10.59552, 0.0]]\n",
      "['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n"
     ]
    }
   ],
   "source": [
    "print denormalize_centers(kmodel.clusterCenters(), scaler_model_dict)\n",
    "print cols_to_cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
