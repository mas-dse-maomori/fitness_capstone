{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create pyspark dataframe from csv\n",
    "def df_from_csv(csv_file):\n",
    "    text = sc.textFile(csv_file)\\\n",
    "        .map(lambda line: line.split(\",\"))\n",
    "    #didn't work with take(1). believe returns \n",
    "    #different object then first()\n",
    "    schema = text.first()\n",
    "    data = text.filter(lambda x: x != schema)\n",
    "    df = sqlContext.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "def change_column_names(df, old_names, new_names):\n",
    "    pass\n",
    "    return df;\n",
    "\n",
    "##input\n",
    "##   string: 2013-02-22 21:38:45\n",
    "##output: \n",
    "##   list: [u'2013', u'02', u'22', u'21', u'38', u'45']\n",
    "def datetime_to_trace_time(timestamp):\n",
    "    #strip double quotes\n",
    "    timestamp = timestamp.replace('\"', '')\n",
    "    r_list = re.split('[- :]', timestamp)\n",
    "    return [int(x) for x in r_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|              \"time\"|\"altitude\"|\"heart_rate\"|   \"latitude\"|  \"longitude\"|     \"speed\"|\"workoutid\"|    \"id\"|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|\"2013-02-22 21:38...|          |   147.00000|51.4394768234|-0.8953504544|6.7140000000|  167479013|13856798|\n",
      "|\"2013-02-22 21:38...|          |   148.00000|51.4392022323|-0.8952703234|9.3492000000|  167479013|13856799|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_run='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_data_with_hr_spd.csv'\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "df_runs = df_from_csv(data_run)\n",
    "print type(df_runs)\n",
    "df_runs.show(2)\n",
    "df_runs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = df_runs.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import nbimporter\n",
    "#for ordered dict for traces implementation\n",
    "from collections import OrderedDict\n",
    "\n",
    "import traces\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "#for timedelta manipulation\n",
    "from math import fabs\n",
    "\n",
    "#kmeans\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from  pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc =SparkContext()\n",
    "\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#%load_ext autotime\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#del min\n",
    "#del max\n",
    "\n",
    "data_run='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_data_with_hr_spd.csv'\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get list of workoutids\n",
    "#woutid_list = df_runs.select('\"workoutid\"').distinct()\\\n",
    "#    .rdd.map(lambda r:r[0]).collect()\n",
    "#woutid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class to create summary stats of workouts\n",
    "class SummaryStat():\n",
    "    data_points = ''\n",
    "    new_col_names = []\n",
    "    #list of tuples. \n",
    "    #   key: column name\n",
    "    #   value: metric (sum/avg/etc)\n",
    "    summary_conf = ''\n",
    "    partition_name = ''\n",
    "    def __init__(self, sql_df, summary_conf = [], \\\n",
    "                partition_name = ''):\n",
    "        if partition_name:\n",
    "            self.partition_name = partition_name\n",
    "        else:\n",
    "            self.partition_name = '\"workoutid\"'\n",
    "        self.data_points = sql_df\n",
    "        if not isinstance(sql_df, DataFrame):\n",
    "            print 'please enter spark dataframe object'\n",
    "            return\n",
    "        #if summary_conf not defined, default to sum for all columns\n",
    "        if not summary_conf:\n",
    "            col_names = self.data_points.schema.names\n",
    "            for i in col_names:\n",
    "                summary_conf.append((i, 'sum'))\n",
    "        else:\n",
    "            if not isinstance(summary_conf, list):\n",
    "                print 'check summary_conf type'\n",
    "                return\n",
    "            for element in summary_conf:\n",
    "                if not isinstance(element, dict):\n",
    "                    print 'check summary_conf type 2'\n",
    "                    return\n",
    "            self.summary_conf = summary_conf\n",
    "        \n",
    "    def describe(self):\n",
    "        print self.data_points.limit(2)\n",
    "\n",
    "    def get_new_col_names(self):\n",
    "        return self.new_col_names\n",
    "        \n",
    "    def generate_summary(self):\n",
    "        #rewnew new_col_names\n",
    "        self.new_col_names = []\n",
    "        #windows function test\n",
    "        r_df = ''\n",
    "        r_list_cols = []\n",
    "        for dic in self.summary_conf:\n",
    "            col_name = dic.keys()[0] #should only have 1 key per dictionary\n",
    "            aggregation_type = dic[col_name]\n",
    "            w = Window.partitionBy(self.partition_name)\n",
    "            if aggregation_type == 'avg':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        avg(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())\n",
    "                self.new_col_names.append(new_col_name)\n",
    "\n",
    "            if aggregation_type == 'sum':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        sum(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())   \n",
    "                self.new_col_names.append(new_col_name)\n",
    "\n",
    "                \n",
    "            if aggregation_type == 'max':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        max(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())    \n",
    "                self.new_col_names.append(new_col_name)\n",
    "                \n",
    "            if aggregation_type == 'min':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        min(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct()) \n",
    "                self.new_col_names.append(new_col_name)\n",
    "                \n",
    "        for df in r_list_cols:\n",
    "            if not r_df:\n",
    "                r_df = df\n",
    "            else:\n",
    "                r_df = r_df.join(df, (r_df[self.partition_name]==df[self.partition_name]), 'inner')\\\n",
    "                    .drop(df[self.partition_name])\n",
    "        self.new_dataframe = r_df\n",
    "        return r_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#minMaxScaler wrapper since originalMin/Max is only implemented in 2.0\n",
    "class scaler_wrapper():\n",
    "    mmModel = ''\n",
    "    originalMin = ''\n",
    "    originalMax = ''   \n",
    "    \n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        self.mmModel = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
    "        self.in_column = inputCol\n",
    "        \n",
    "    def get_input_col_name(self):\n",
    "        return self.mmModel.getInputCol()\n",
    "\n",
    "    def getMax(self):\n",
    "        return self.mmModel.getMax()\n",
    "        \n",
    "    def getMin(self):\n",
    "        return self.mmModel.getMin()\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'describe'\n",
    "    \n",
    "    def fit(self, df):\n",
    "        col = self.mmModel.getInputCol()\n",
    "        self.originalMin = df.select(col).rdd.flatMap(lambda x: x[0]).min()\n",
    "        self.originalMax = df.select(col).rdd.flatMap(lambda x: x[0]).max()\n",
    "        return self.mmModel.fit(df)\n",
    "    \n",
    "    #denormalize the value\n",
    "    def denormalize(self, value):\n",
    "        v = (value-self.getMin())*\\\n",
    "            (self.originalMax - self.originalMin)*\\\n",
    "            (self.getMax()-self.getMin()) + self.originalMin\n",
    "        if v or v == 0:\n",
    "            return v\n",
    "        else:\n",
    "            return -999\n",
    "        \n",
    "    def normalize(self, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#change column type\n",
    "def change_type_cols_double(df, list_cols):\n",
    "    for col_name in list_cols:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(\"double\"))\n",
    "    return df.na.fill(0)\n",
    "\n",
    "#vectorize the column\n",
    "#keeps the original name\n",
    "def vectorize_columns(df, list_cols):\n",
    "    tmp_col_name = 'temp'\n",
    "    for col_name in list_cols:\n",
    "        vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "        df = df.withColumn(tmp_col_name, vectorize(df[col_name])).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "    return df\n",
    "\n",
    "##scaler wrapper usage\n",
    "#generate normalized dataframe\n",
    "#keep the original column names\n",
    "#returns:\n",
    "#   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "def normalize_df(df, list_cols):\n",
    "    tmp_col_name = 'temp'\n",
    "    r_dict = {}\n",
    "    index = 0\n",
    "    for col_name in list_cols:\n",
    "        #scaler is the wrapper instance\n",
    "        scaler = scaler_wrapper(inputCol=col_name, outputCol=tmp_col_name)\n",
    "        scalerModel = scaler.fit(df)\n",
    "        df = scalerModel.transform(df).drop(col_name)\\\n",
    "            .withColumnRenamed(tmp_col_name, col_name)\n",
    "        #r_dict[col_name] = scaler\n",
    "        r_dict[index] = scaler\n",
    "        index+=1\n",
    "    return df, r_dict\n",
    "\n",
    "# MinMaxScaler doesn't have originalMin(only supported in 2.0). made one above cell with wrapper class\n",
    "# #generate normalized dataframe\n",
    "# #keep the original column names\n",
    "# #returns:\n",
    "# #   (df, {col_name:scalerModel, col_name:scalerModel,....})\n",
    "# def normalize_df(df, list_cols):\n",
    "#     tmp_col_name = 'temp'\n",
    "#     r_dict = {}\n",
    "#     for col_name in list_cols:\n",
    "#         scaler = MinMaxScaler(inputCol=col_name, outputCol=tmp_col_name)\n",
    "#         scalerModel = scaler.fit(df)\n",
    "#         df = scalerModel.transform(df).drop(col_name)\\\n",
    "#             .withColumnRenamed(tmp_col_name, col_name)\n",
    "#         r_dict[col_name] = [scaler, scalerModel]\n",
    "#     return df, r_dict\n",
    "\n",
    "#input: (MLLIB KMEANS)\n",
    "#   cols_to_cluster: list of column names to cluster\n",
    "# def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "#     vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "#         outputCol='features')\n",
    "#     df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "#     df_to_cluster = df_to_cluster.select('features')\n",
    "#     df_to_cluster = df_to_cluster.rdd\\\n",
    "#         .map(lambda row : Vectors.dense([item for item in row]))\n",
    "#     clusters = KMeans.train(df_to_cluster, num_clusters,\\\n",
    "#                            maxIterations = 10,\\\n",
    "#                         initializationMode=\"random\")\n",
    "    \n",
    "#     return clusters\n",
    "\n",
    "\n",
    "#ML KMEANS\n",
    "#returns KMeanModel, df_to_cluster\n",
    "def cluster_summary_df(df, cols_to_cluster, num_clusters = 2):\n",
    "    vecAssembler = VectorAssembler(inputCols=cols_to_cluster,\\\n",
    "        outputCol='features')\n",
    "    df_to_cluster = vecAssembler.transform(df.na.fill(0))\n",
    "    df_to_cluster = df_to_cluster.select('features')\n",
    "    #df_to_cluster = df_to_cluster.rdd\\\n",
    "     #   .map(lambda row : Vectors.dense([item for item in row]))\n",
    "    kmeans = KMeans(k=num_clusters, seed=1)\n",
    "    model = kmeans.fit(df_to_cluster)\n",
    "    return model, df_to_cluster\n",
    "    \n",
    "#returns clusters. show cluster centerse by clusters.clusterCenters.\n",
    "def cluster_df(df, cols_to_normalize, num_clusters = 2,  cols_to_cluster = []):\n",
    "    if not cols_to_cluster:\n",
    "        cols_to_cluster = cols_to_normalize\n",
    "    df = change_type_cols_double(df, cols_to_normalize)\n",
    "    df = vectorize_columns(df, cols_to_normalize)\n",
    "    df, scalerModels = normalize_df(df, cols_to_cluster)\n",
    "    r_obj = cluster_summary_df(df, cols_to_cluster, num_clusters)\n",
    "    #r_obj = [KMeanModel, new_df]\n",
    "    return r_obj[0], r_obj[1], scalerModels\n",
    "\n",
    "def denormalize_centers(list_centers, scaler_models):\n",
    "    new_centers = []\n",
    "    for center in list_centers:\n",
    "        n_center = []\n",
    "        for c in range(len(center)):\n",
    "            n_center.append(scaler_models[c].denormalize(center[c]))\n",
    "        new_centers.append(n_center)\n",
    "    return new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n",
      "+------------------+-----------+-----------+--------------+\n",
      "|  avg_\"heart_rate\"|avg_\"speed\"|\"workoutid\"|avg_\"altitude\"|\n",
      "+------------------+-----------+-----------+--------------+\n",
      "|             103.0|        0.0|  165504204|          null|\n",
      "|150.06666666666666|   10.59552|  167479013|          null|\n",
      "+------------------+-----------+-----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define columns to cluster on (they will be normalized)\n",
    "#cols_to_cluster = ['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n",
    "#create summary stats for workouts\n",
    "\n",
    "s_conf = [{'\"heart_rate\"': 'avg'}, {'\"speed\"': 'avg'}, {'\"altitude\"': 'avg'}]\n",
    "\n",
    "#obj = SummaryStat(df_runs, summary_conf=s_conf)\n",
    "obj = SummaryStat(sub_df, summary_conf=s_conf)\n",
    "\n",
    "#obj.describe()\n",
    "sumstats = obj.generate_summary()\n",
    "cols_to_cluster = obj.get_new_col_names()\n",
    "print cols_to_cluster\n",
    "sumstats.show()\n",
    "sumstats.count()\n",
    "#clusters = cluster_df(sumstats, cols_to_cluster, 5)\n",
    "#clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_obj = cluster_df(sumstats, cols_to_cluster, num_clusters=2) \n",
    "kmodel = r_obj[0]\n",
    "new_df = r_obj[1]\n",
    "scaler_model_dict = r_obj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.0, 0.0, 0.0], [150.06666666666666, 10.59552, 0.0]]\n",
      "['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n"
     ]
    }
   ],
   "source": [
    "print denormalize_centers(kmodel.clusterCenters(), scaler_model_dict)\n",
    "print cols_to_cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.02985281,  0.68377991,  0.05522858]),\n",
       " array([ 0.04080238,  0.60569136,  0.05948706]),\n",
       " array([ 0.03575399,  0.470584  ,  0.05394333]),\n",
       " array([ 0.03248862,  0.36756594,  0.04606859]),\n",
       " array([ 0.03308665,  0.53706301,  0.05880067])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cols_to_change = ['\"altitude\"', '\"heart_rate\"', '\"speed\"']\n",
    "clusters = cluster_df(df_runs, list_cols_to_change, 5)\n",
    "clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(\"time\"=u'\"2013-02-22 21:38:45\"', \"altitude\"=u'', \"heart_rate\"=u'147.00000', \"latitude\"=u'51.4394768234', \"longitude\"=u'-0.8953504544', \"speed\"=u'6.7140000000', \"workoutid\"=u'167479013', \"id\"=u'13856798'),\n",
       " Row(\"time\"=u'\"2013-02-22 21:38:57\"', \"altitude\"=u'', \"heart_rate\"=u'148.00000', \"latitude\"=u'51.4392022323', \"longitude\"=u'-0.8952703234', \"speed\"=u'9.3492000000', \"workoutid\"=u'167479013', \"id\"=u'13856799')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df_runs.limit(20)\n",
    "sub_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------+------------------+\n",
      "|  avg_\"heart_rate\"|       avg_\"speed\"|\"workoutid\"|    avg_\"altitude\"|\n",
      "+------------------+------------------+-----------+------------------+\n",
      "|            158.24| 9.455241600000008|  171117914|169.40080000000015|\n",
      "|           148.138|11.577304799999983|  219630823| 41.31679999999993|\n",
      "|           152.662|         7.3092888|  325622855|21.065600000000018|\n",
      "|165.95172413793102| 7.994391724137932|  430188558| 289.2551724137944|\n",
      "|           164.334|11.034640800000005|  325634772|245.03999999999948|\n",
      "|           162.708| 9.122702400000005|  358388284| 88.63919999999996|\n",
      "|           144.008|10.763495999999996|  513660945|-5.325999999999993|\n",
      "|           151.172|12.929040000000002|  532228357| 82.64479999999995|\n",
      "|           148.964|12.785579999999992|  628219304| 42.66640000000001|\n",
      "|127.00862068965517| 8.785924137931035|  436651654|119.02758620689657|\n",
      "| 145.0976430976431|10.043393939393928|  547204459| 1.528619528619532|\n",
      "|150.87179487179486|               0.0|  255210417|              null|\n",
      "|            147.46|11.928600000000008|  472955225|148.55759999999995|\n",
      "|118.47909967845659|  9.70113183279742|  498615552| 14.83729903536978|\n",
      "|            151.35|14.503485600000007|  531303446|29.315200000000008|\n",
      "| 147.9141914191419|10.323766336633664|  557166841| 7.599339933993405|\n",
      "|123.82790697674419| 8.830615813953482|  627799241|127.99720930232552|\n",
      "|            163.73| 9.928512000000012|  103580393| 75.65919999999993|\n",
      "|           150.482|12.225124800000012|  310761859| 236.2843999999999|\n",
      "|           159.022|13.919832000000016|  402929691|189.37239999999997|\n",
      "+------------------+------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probably smarter to go from top to down so creating summary will\n",
    "#be O(n), where n is number of rows in df\n",
    "s_conf = [{'\"heart_rate\"': 'avg'}, {'\"speed\"': 'avg'}, {'\"altitude\"': 'avg'}]\n",
    "\n",
    "obj = SummaryStat(df_runs, summary_conf=s_conf)\n",
    "#obj.describe()\n",
    "sumstats = obj.generate_summary()\n",
    "sumstats.show()\n",
    "sumstats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get some column names that we want to cluster\n",
    "print sumstats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = []\n",
    "# for s in s_conf:\n",
    "#     for key in s:\n",
    "#         cols.append(key)\n",
    "# print cols\n",
    "\n",
    "cols=['avg_\"heart_rate\"', 'avg_\"speed\"', 'avg_\"altitude\"']\n",
    "\n",
    "clusters = cluster_summary_df(sumstats, cols, num_clusters = 8)\n",
    "clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(\"time\",StringType,true),StructField(\"altitude\",StringType,true),StructField(\"heart_rate\",StringType,true),StructField(\"latitude\",StringType,true),StructField(\"longitude\",StringType,true),StructField(\"speed\",StringType,true),StructField(\"workoutid\",StringType,true),StructField(\"id\",StringType,true)))\n",
      "\n",
      "\n",
      "+--------------------+-------------+-------------+-----------+--------+----------+--------------------+--------------------+\n",
      "|              \"time\"|   \"latitude\"|  \"longitude\"|\"workoutid\"|    \"id\"|\"altitude\"|        \"heart_rate\"|             \"speed\"|\n",
      "+--------------------+-------------+-------------+-----------+--------+----------+--------------------+--------------------+\n",
      "|\"2013-02-22 21:38...|51.4394768234|-0.8953504544|  167479013|13856798|     [0.5]|[0.9206349206349206]|[0.5020188425302826]|\n",
      "|\"2013-02-22 21:38...|51.4392022323|-0.8952703234|  167479013|13856799|     [0.5]|[0.9365079365079365]| [0.699057873485868]|\n",
      "|\"2013-02-22 21:39...|51.4390821196|-0.8953502029|  167479013|13856800|     [0.5]|[0.9365079365079365]| [0.794885598923284]|\n",
      "|\"2013-02-22 21:39...|51.4388703089|-0.8954045177|  167479013|13856801|     [0.5]|[0.9523809523809523]|               [1.0]|\n",
      "|\"2013-02-22 21:39...|51.4386848174|-0.8952745982|  167479013|13856802|     [0.5]|[0.9841269841269841]|[0.9456258411843876]|\n",
      "|\"2013-02-22 21:39...|51.4385818038|-0.8951149229|  167479013|13856803|     [0.5]|               [1.0]|[0.8697173620457604]|\n",
      "|\"2013-02-22 21:39...|51.4383811411|-0.8946192171|  167479013|13856804|     [0.5]|[0.9841269841269841]|[0.8263795423956931]|\n",
      "|\"2013-02-22 21:39...|51.4383305982|-0.8943347353|  167479013|13856805|     [0.5]|[0.9841269841269841]|[0.8395693135935397]|\n",
      "|\"2013-02-22 21:39...|51.4382817317|-0.8940684423|  167479013|13856806|     [0.5]|[0.9682539682539683]|[0.8602960969044414]|\n",
      "|\"2013-02-22 21:39...|51.4382587653|-0.8938501775|  167479013|13856807|     [0.5]|               [1.0]|[0.8511440107671602]|\n",
      "|\"2013-02-22 21:39...|51.4382402413|-0.8936407138|  167479013|13856808|     [0.5]|[0.9841269841269841]|[0.8145356662180349]|\n",
      "|\"2013-02-22 21:40...|51.4383392315|-0.8935162425|  167479013|13856809|     [0.5]|[0.9682539682539683]|[0.6874831763122476]|\n",
      "|\"2013-02-22 21:40...|51.4383939654|-0.8936719783|  167479013|13856810|     [0.5]|[0.9682539682539683]|[0.7028263795423956]|\n",
      "|\"2013-02-22 21:40...|51.4383674785|-0.8938780893|  167479013|13856811|     [0.5]|[0.9841269841269841]|  [0.74185733512786]|\n",
      "|\"2013-02-22 21:40...|51.4383420814|-0.8940940071|  167479013|13856812|     [0.5]|[0.9682539682539683]|[0.7483176312247644]|\n",
      "|\"2013-02-22 20:52...|51.4382775407|-0.8940775786|  165504204|13856813|     [0.5]|               [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:52...|51.4382775407|-0.8940775786|  165504204|13856814|     [0.5]|               [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:53...|51.4382775407|-0.8940775786|  165504204|13856815|     [0.5]|[0.1746031746031746]|               [0.0]|\n",
      "|\"2013-02-22 20:53...|51.4382775407|-0.8940775786|  165504204|13856816|     [0.5]|[0.42857142857142...|               [0.0]|\n",
      "|\"2013-02-22 20:53...|51.4382775407|-0.8940775786|  165504204|13856817|     [0.5]|[0.5079365079365079]|               [0.0]|\n",
      "+--------------------+-------------+-------------+-----------+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print sub_df.schema\n",
    "list_cols_to_change = ['\"altitude\"', '\"heart_rate\"', '\"speed\"']\n",
    "newd = change_type_cols_double(sub_df, list_cols_to_change)\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "\n",
    "#vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "#df = df.withColumn('temp', vectorize(sub_df[col_name]))\n",
    "\n",
    "dd = vectorize_columns(newd, list_cols_to_change)\n",
    "\n",
    "#print dd.show()\n",
    "#print '\\n'\n",
    "#normalize_df(dd, ['\"speed\"']).show()\n",
    "#scaler = MinMaxScaler(inputCol='\"speed\"', outputCol=\"scaledspeed\")\n",
    "#scalerModel = scaler.fit(dd)\n",
    "#scaledData = scalerModel.transform(dd)\n",
    "#print scaledData.show()\n",
    "normalize_df(dd, list_cols_to_change).show()\n",
    "#normalize_df(dd, ['\"speed\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-547ec4caed7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorUDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\"speed\"'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/mllib/linalg/__init__.pyc\u001b[0m in \u001b[0;36mdense\u001b[0;34m(*elements)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;31m# it's list, numpy.array or other iterable object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/mllib/linalg/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ar)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "vectorize = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "print Vectors.dense(sub_df['\"speed\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(\"time\",StringType,true),StructField(\"altitude\",StringType,true),StructField(\"heart_rate\",StringType,true),StructField(\"latitude\",StringType,true),StructField(\"longitude\",StringType,true),StructField(\"speed\",StringType,true),StructField(\"workoutid\",StringType,true),StructField(\"id\",StringType,true)))\n",
      "StructType(List(StructField(\"time\",StringType,true),StructField(\"altitude\",StringType,true),StructField(\"heart_rate\",StringType,true),StructField(\"latitude\",StringType,true),StructField(\"longitude\",StringType,true),StructField(\"speed\",DoubleType,true),StructField(\"workoutid\",StringType,true),StructField(\"id\",StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print sub_df.schema\n",
    "sub_df = sub_df.withColumn('\"speed\"', sub_df['\"speed\"'].cast(\"double\"))\n",
    "print sub_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(\"time\",StringType,true),StructField(\"altitude\",StringType,true),StructField(\"heart_rate\",StringType,true),StructField(\"latitude\",StringType,true),StructField(\"longitude\",StringType,true),StructField(\"speed\",DoubleType,true),StructField(\"workoutid\",StringType,true),StructField(\"id\",StringType,true),StructField(speed,VectorUDT,true)))\n"
     ]
    }
   ],
   "source": [
    "dd = sub_df.withColumn('speed', vectorize(sub_df['\"speed\"']))\n",
    "#print dd.show()\n",
    "print dd.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+-------------+-------------+-------+-----------+--------+---------+--------------------+\n",
      "|              \"time\"|\"altitude\"|\"heart_rate\"|   \"latitude\"|  \"longitude\"|\"speed\"|\"workoutid\"|    \"id\"|    speed|         scaledspeed|\n",
      "+--------------------+----------+------------+-------------+-------------+-------+-----------+--------+---------+--------------------+\n",
      "|\"2013-02-22 21:38...|          |   147.00000|51.4394768234|-0.8953504544|  6.714|  167479013|13856798|  [6.714]|[0.5020188425302826]|\n",
      "|\"2013-02-22 21:38...|          |   148.00000|51.4392022323|-0.8952703234| 9.3492|  167479013|13856799| [9.3492]| [0.699057873485868]|\n",
      "|\"2013-02-22 21:39...|          |   148.00000|51.4390821196|-0.8953502029|10.6308|  167479013|13856800|[10.6308]| [0.794885598923284]|\n",
      "|\"2013-02-22 21:39...|          |   149.00000|51.4388703089|-0.8954045177| 13.374|  167479013|13856801| [13.374]|               [1.0]|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4386848174|-0.8952745982|12.6468|  167479013|13856802|[12.6468]|[0.9456258411843876]|\n",
      "|\"2013-02-22 21:39...|          |   152.00000|51.4385818038|-0.8951149229|11.6316|  167479013|13856803|[11.6316]|[0.8697173620457604]|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4383811411|-0.8946192171| 11.052|  167479013|13856804| [11.052]|[0.8263795423956931]|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4383305982|-0.8943347353|11.2284|  167479013|13856805|[11.2284]|[0.8395693135935397]|\n",
      "|\"2013-02-22 21:39...|          |   150.00000|51.4382817317|-0.8940684423|11.5056|  167479013|13856806|[11.5056]|[0.8602960969044414]|\n",
      "|\"2013-02-22 21:39...|          |   152.00000|51.4382587653|-0.8938501775|11.3832|  167479013|13856807|[11.3832]|[0.8511440107671602]|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4382402413|-0.8936407138|10.8936|  167479013|13856808|[10.8936]|[0.8145356662180349]|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383392315|-0.8935162425| 9.1944|  167479013|13856809| [9.1944]|[0.6874831763122476]|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383939654|-0.8936719783| 9.3996|  167479013|13856810| [9.3996]|[0.7028263795423956]|\n",
      "|\"2013-02-22 21:40...|          |   151.00000|51.4383674785|-0.8938780893| 9.9216|  167479013|13856811| [9.9216]|  [0.74185733512786]|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383420814|-0.8940940071| 10.008|  167479013|13856812| [10.008]|[0.7483176312247644]|\n",
      "|\"2013-02-22 20:52...|          |    89.00000|51.4382775407|-0.8940775786|    0.0|  165504204|13856813|    [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:52...|          |    89.00000|51.4382775407|-0.8940775786|    0.0|  165504204|13856814|    [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:53...|          |   100.00000|51.4382775407|-0.8940775786|    0.0|  165504204|13856815|    [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:53...|          |   116.00000|51.4382775407|-0.8940775786|    0.0|  165504204|13856816|    [0.0]|               [0.0]|\n",
      "|\"2013-02-22 20:53...|          |   121.00000|51.4382775407|-0.8940775786|    0.0|  165504204|13856817|    [0.0]|               [0.0]|\n",
      "+--------------------+----------+------------+-------------+-------------+-------+-----------+--------+---------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol='speed', outputCol=\"scaledspeed\")\n",
    "scalerModel = scaler.fit(dd)\n",
    "scaledData = scalerModel.transform(dd)\n",
    "print scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------testing area-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#windows function test\n",
    "w = Window.partitionBy('\"workoutid\"')\n",
    "tmp = sub_df.select('\"heart_rate\"', '\"workoutid\"', \\\n",
    "              sum('\"heart_rate\"').over(w).alias(\"sum\")\\\n",
    "                )\n",
    "\n",
    "tmp[['\"heart_rate\"', '\"workoutid\"', 'sum']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#windows function test\n",
    "w = Window.partitionBy('\"workoutid\"')\n",
    "tmp = sub_df.select('\"heart_rate\"', '\"speed\"', '\"workoutid\"', \\\n",
    "              sum('\"heart_rate\"').over(w).alias(\"sum\")\\\n",
    "              , avg('\"speed\"').over(w).alias(\"spd\")\\\n",
    "                )\n",
    "\n",
    "tmp[['\"workoutid\"', 'spd', 'sum']].distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler(inputCol = '\"speed\"', outputCol = 'scaled')\n",
    "#scaled_df = mmscaler.fit(sub_df)\n",
    "sub_df_wo_time = sub_df.drop('\"time\"').drop('\"altitude\"')\n",
    "sub_df_wo_time = sub_df_wo_time.withColumn('\"speed\"', sub_df_wo_time['\"speed\"'].cast(\"double\"))\n",
    "#print sub_df_wo_time.na.fill(0).show()\n",
    "#sub_df_wo_time = sub_df_wo_time.rdd.map(lambda row: Vectors.dense([item for item in row]))\n",
    "print sub_df_wo_time.describe()\n",
    "print sub_df_wo_time.show()\n",
    "#row['\"altitude\"'] = 0\n",
    "#for item in row:\n",
    "#   print type(item), item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(\n",
    "#   inputCols=[\"DF_column\"], outputCol=\"features\"\n",
    "# )\n",
    "\n",
    "# assembled = assembler.transform(df)\n",
    "\n",
    "# scaler = StandardScaler(\n",
    "#   inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "#   withStd=True, withMean=False\n",
    "# ).fit(assembled)\n",
    "\n",
    "# scaler.transform(assembled)\n",
    "\n",
    "assembler = VectorAssembler(\\\n",
    "            inputCols = ['\"speed\"'],\n",
    "            outputCol = ['scaled'])\n",
    "assembled = assembler.transform(sub_df_wo_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='\"speed\"', outputCol=\"scaledRevenue\")\n",
    "scalerModel = scaler.fit(sub_df_wo_time)\n",
    "scaledData = scalerModel.transform(sub_df_wo_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+-------------+-------------+-------------+-----------+--------+\n",
      "|              \"time\"|\"altitude\"|\"heart_rate\"|   \"latitude\"|  \"longitude\"|      \"speed\"|\"workoutid\"|    \"id\"|\n",
      "+--------------------+----------+------------+-------------+-------------+-------------+-----------+--------+\n",
      "|\"2013-02-22 21:38...|          |   147.00000|51.4394768234|-0.8953504544| 6.7140000000|  167479013|13856798|\n",
      "|\"2013-02-22 21:38...|          |   148.00000|51.4392022323|-0.8952703234| 9.3492000000|  167479013|13856799|\n",
      "|\"2013-02-22 21:39...|          |   148.00000|51.4390821196|-0.8953502029|10.6308000000|  167479013|13856800|\n",
      "|\"2013-02-22 21:39...|          |   149.00000|51.4388703089|-0.8954045177|13.3740000000|  167479013|13856801|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4386848174|-0.8952745982|12.6468000000|  167479013|13856802|\n",
      "|\"2013-02-22 21:39...|          |   152.00000|51.4385818038|-0.8951149229|11.6316000000|  167479013|13856803|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4383811411|-0.8946192171|11.0520000000|  167479013|13856804|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4383305982|-0.8943347353|11.2284000000|  167479013|13856805|\n",
      "|\"2013-02-22 21:39...|          |   150.00000|51.4382817317|-0.8940684423|11.5056000000|  167479013|13856806|\n",
      "|\"2013-02-22 21:39...|          |   152.00000|51.4382587653|-0.8938501775|11.3832000000|  167479013|13856807|\n",
      "|\"2013-02-22 21:39...|          |   151.00000|51.4382402413|-0.8936407138|10.8936000000|  167479013|13856808|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383392315|-0.8935162425| 9.1944000000|  167479013|13856809|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383939654|-0.8936719783| 9.3996000000|  167479013|13856810|\n",
      "|\"2013-02-22 21:40...|          |   151.00000|51.4383674785|-0.8938780893| 9.9216000000|  167479013|13856811|\n",
      "|\"2013-02-22 21:40...|          |   150.00000|51.4383420814|-0.8940940071|10.0080000000|  167479013|13856812|\n",
      "|\"2013-02-22 20:52...|          |    89.00000|51.4382775407|-0.8940775786| 0.0000000000|  165504204|13856813|\n",
      "|\"2013-02-22 20:52...|          |    89.00000|51.4382775407|-0.8940775786| 0.0000000000|  165504204|13856814|\n",
      "|\"2013-02-22 20:53...|          |   100.00000|51.4382775407|-0.8940775786| 0.0000000000|  165504204|13856815|\n",
      "|\"2013-02-22 20:53...|          |   116.00000|51.4382775407|-0.8940775786| 0.0000000000|  165504204|13856816|\n",
      "|\"2013-02-22 20:53...|          |   121.00000|51.4382775407|-0.8940775786| 0.0000000000|  165504204|13856817|\n",
      "+--------------------+----------+------------+-------------+-------------+-------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='\"heart_rate\"', outputCol=\"scaledRevenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Input column \"heart_rate\" must be a vector column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-95d3709f674e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/ml/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/momori/app/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: Input column \"heart_rate\" must be a vector column'"
     ]
    }
   ],
   "source": [
    "scalerModel = scaler.fit(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
