{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import nbimporter\n",
    "#for ordered dict for traces implementation\n",
    "from collections import OrderedDict\n",
    "\n",
    "import traces\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import sources.endomondolib as endo\n",
    "#import sources.pysparkconvenience as ps\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "#for timedelta manipulation\n",
    "from math import fabs\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "#%load_ext autotime\n",
    "from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "del min\n",
    "del max\n",
    "\n",
    "data_run='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_data_with_hr_spd.csv'\n",
    "data_users='/Users/momori/dse/maomori/thesis/git/fitness_capstone/data/run_with_hr_users.csv'\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create pyspark dataframe from csv\n",
    "def df_from_csv(csv_file):\n",
    "    text = sc.textFile(csv_file)\\\n",
    "        .map(lambda line: line.split(\",\"))\n",
    "    #didn't work with take(1). believe returns \n",
    "    #different object then first()\n",
    "    schema = text.first()\n",
    "    data = text.filter(lambda x: x != schema)\n",
    "    df = sqlContext.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "def change_column_names(df, old_names, new_names):\n",
    "    pass\n",
    "    return df;\n",
    "\n",
    "##input\n",
    "##   string: 2013-02-22 21:38:45\n",
    "##output: \n",
    "##   list: [u'2013', u'02', u'22', u'21', u'38', u'45']\n",
    "def datetime_to_trace_time(timestamp):\n",
    "    #strip double quotes\n",
    "    timestamp = timestamp.replace('\"', '')\n",
    "    r_list = re.split('[- :]', timestamp)\n",
    "    return [int(x) for x in r_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|              \"time\"|\"altitude\"|\"heart_rate\"|   \"latitude\"|  \"longitude\"|     \"speed\"|\"workoutid\"|    \"id\"|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "|\"2013-02-22 21:38...|          |   147.00000|51.4394768234|-0.8953504544|6.7140000000|  167479013|13856798|\n",
      "|\"2013-02-22 21:38...|          |   148.00000|51.4392022323|-0.8952703234|9.3492000000|  167479013|13856799|\n",
      "+--------------------+----------+------------+-------------+-------------+------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_runs = df_from_csv(data_run)\n",
    "print type(df_runs)\n",
    "df_runs.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get list of workoutids\n",
    "#woutid_list = df_runs.select('\"workoutid\"').distinct()\\\n",
    "#    .rdd.map(lambda r:r[0]).collect()\n",
    "#woutid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class to create summary stats of workouts\n",
    "class SummaryStat():\n",
    "    data_points = ''\n",
    "    \n",
    "    #list of tuples. \n",
    "    #   key: column name\n",
    "    #   value: metric (sum/avg/etc)\n",
    "    summary_conf = ''\n",
    "    partition_name = ''\n",
    "    def __init__(self, sql_df, summary_conf = [], \\\n",
    "                partition_name = ''):\n",
    "        if partition_name:\n",
    "            self.partition_name = partition_name\n",
    "        else:\n",
    "            self.partition_name = '\"workoutid\"'\n",
    "        self.data_points = sql_df\n",
    "        if not isinstance(sql_df, DataFrame):\n",
    "            print 'please enter spark dataframe object'\n",
    "            return\n",
    "        #if summary_conf not defined, default to sum for all columns\n",
    "        if not summary_conf:\n",
    "            col_names = self.data_points.schema.names\n",
    "            for i in col_names:\n",
    "                summary_conf.append((i, 'sum'))\n",
    "        else:\n",
    "            if not isinstance(summary_conf, list):\n",
    "                print 'check summary_conf type'\n",
    "                return\n",
    "            for element in summary_conf:\n",
    "                print type(element), element\n",
    "                if not isinstance(element, dict):\n",
    "                    print 'check summary_conf type 2'\n",
    "                    return\n",
    "            self.summary_conf = summary_conf\n",
    "        \n",
    "    def describe(self):\n",
    "        print self.data_points.limit(2)\n",
    "\n",
    "    def generate_summary(self):\n",
    "        #windows function test\n",
    "        r_df = ''\n",
    "        r_list_cols = []\n",
    "        for dic in self.summary_conf:\n",
    "            col_name = dic.keys()[0] #should only have 1 key per dictionary\n",
    "            aggregation_type = dic[col_name]\n",
    "            print col_name\n",
    "            w = Window.partitionBy(self.partition_name)\n",
    "            if aggregation_type == 'avg':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        avg(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())\n",
    "            if aggregation_type == 'sum':\n",
    "                new_col_name = aggregation_type + '_' + col_name\n",
    "                tmp = self.data_points.select(col_name, self.partition_name,\\\n",
    "                                        sum(col_name).over(w).alias(new_col_name)\\\n",
    "                                            )\n",
    "                r_list_cols.append(tmp[[new_col_name, self.partition_name]].distinct())                \n",
    "        \n",
    "        for df in r_list_cols:\n",
    "            if not r_df:\n",
    "                r_df = df\n",
    "            else:\n",
    "                print 'joining'\n",
    "                r_df = r_df.join(df, (r_df[self.partition_name]==df[self.partition_name]), 'inner')\\\n",
    "                    .drop(df[self.partition_name])\n",
    "        return r_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(\"time\"=u'\"2013-02-22 21:38:45\"', \"altitude\"=u'', \"heart_rate\"=u'147.00000', \"latitude\"=u'51.4394768234', \"longitude\"=u'-0.8953504544', \"speed\"=u'6.7140000000', \"workoutid\"=u'167479013', \"id\"=u'13856798'),\n",
       " Row(\"time\"=u'\"2013-02-22 21:38:57\"', \"altitude\"=u'', \"heart_rate\"=u'148.00000', \"latitude\"=u'51.4392022323', \"longitude\"=u'-0.8952703234', \"speed\"=u'9.3492000000', \"workoutid\"=u'167479013', \"id\"=u'13856799')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df_runs.limit(20)\n",
    "sub_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'> {'\"heart_rate\"': 'avg'}\n",
      "<type 'dict'> {'\"speed\"': 'sum'}\n",
      "\"heart_rate\"\n",
      "\"speed\"\n",
      "joining\n",
      "+------------------+------------------+-----------+\n",
      "|  avg_\"heart_rate\"|       sum_\"speed\"|\"workoutid\"|\n",
      "+------------------+------------------+-----------+\n",
      "|             103.0|               0.0|  165504204|\n",
      "|150.06666666666666|158.93280000000001|  167479013|\n",
      "+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probably smarter to go from top to down so creating summary will\n",
    "#be O(n), where n is number of rows in df\n",
    "s_conf = [{'\"heart_rate\"': 'avg'}, {'\"speed\"': 'sum'}]\n",
    "\n",
    "obj = SummaryStat(sub_df, summary_conf=s_conf)\n",
    "#obj.describe()\n",
    "obj.generate_summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[\"time\": string, \"altitude\": string, \"heart_rate\": string, \"latitude\": string, \"longitude\": string, \"speed\": string, \"workoutid\": string, \"id\": string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.limit(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------+\n",
      "|\"heart_rate\"|\"workoutid\"|   sum|\n",
      "+------------+-----------+------+\n",
      "|    89.00000|  165504204| 515.0|\n",
      "|    89.00000|  165504204| 515.0|\n",
      "|   100.00000|  165504204| 515.0|\n",
      "|   116.00000|  165504204| 515.0|\n",
      "|   121.00000|  165504204| 515.0|\n",
      "|   147.00000|  167479013|2251.0|\n",
      "|   148.00000|  167479013|2251.0|\n",
      "|   148.00000|  167479013|2251.0|\n",
      "|   149.00000|  167479013|2251.0|\n",
      "|   151.00000|  167479013|2251.0|\n",
      "|   152.00000|  167479013|2251.0|\n",
      "|   151.00000|  167479013|2251.0|\n",
      "|   151.00000|  167479013|2251.0|\n",
      "|   150.00000|  167479013|2251.0|\n",
      "|   152.00000|  167479013|2251.0|\n",
      "|   151.00000|  167479013|2251.0|\n",
      "|   150.00000|  167479013|2251.0|\n",
      "|   150.00000|  167479013|2251.0|\n",
      "|   151.00000|  167479013|2251.0|\n",
      "|   150.00000|  167479013|2251.0|\n",
      "+------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#windows function test\n",
    "w = Window.partitionBy('\"workoutid\"')\n",
    "tmp = sub_df.select('\"heart_rate\"', '\"workoutid\"', \\\n",
    "              sum('\"heart_rate\"').over(w).alias(\"sum\")\\\n",
    "                )\n",
    "\n",
    "tmp[['\"heart_rate\"', '\"workoutid\"', 'sum']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|\"workoutid\"|     spd|   sum|\n",
      "+-----------+--------+------+\n",
      "|  165504204|     0.0| 515.0|\n",
      "|  167479013|10.59552|2251.0|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#windows function test\n",
    "w = Window.partitionBy('\"workoutid\"')\n",
    "tmp = sub_df.select('\"heart_rate\"', '\"speed\"', '\"workoutid\"', \\\n",
    "              sum('\"heart_rate\"').over(w).alias(\"sum\")\\\n",
    "              , avg('\"speed\"').over(w).alias(\"spd\")\\\n",
    "                )\n",
    "\n",
    "tmp[['\"workoutid\"', 'spd', 'sum']].distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
