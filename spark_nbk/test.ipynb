{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###pyspark basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/momori/Documents/masa_DSE2/maomori/thesis/git/fitness_capstone/spark_nbk\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"/Users/momori/dse/maomori/thesis/git/fitness_capstone/spark_nbk/data/ml-100k/u.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#Get the data here http://grouplens.org/datasets/movielens/\n",
    "movielens = sc.textFile(data_loc)\n",
    "\n",
    "print movielens.first() #u'196\\t242\\t3\\t881250949'\n",
    "print movielens.count() #100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02931607524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Clean up the data by splitting it\n",
    "#Movielens readme says the data is split by tabs and\n",
    "#is user product rating timestamp\n",
    "clean_data = movielens.map(lambda x:x.split('\\t'))\n",
    "\n",
    "#As an example, extract just the ratings to its own RDD\n",
    "#rate.first() is 3\n",
    "rate = clean_data.map(lambda y: int(y[2]))\n",
    "rate.mean() #Avg rating is 3.52986\n",
    "\n",
    "#Extract just the users\n",
    "users = clean_data.map(lambda y: int(y[0]))\n",
    "users.distinct().count() #943 users\n",
    "\n",
    "#You don't have to extract data to its own RDD\n",
    "#This command counts the distinct movies\n",
    "#There are 1,682 movies\n",
    "clean_data.map(lambda y: int(y[1])).distinct().count() \n",
    "\n",
    "#Need to import three functions / objects from the MLlib\n",
    "from pyspark.mllib.recommendation\\\n",
    "    import ALS,MatrixFactorizationModel, Rating\n",
    "\n",
    "#We'll need to map the movielens data to a Ratings object \n",
    "#A Ratings object is made up of (user, item, rating)\n",
    "mls = movielens.map(lambda l: l.split('\\t'))\n",
    "ratings = mls.map(lambda x: Rating(int(x[0]),\\\n",
    "    int(x[1]), float(x[2])))\n",
    "\n",
    "#Need a training and test set\n",
    "train, test = ratings.randomSplit([0.7,0.3],7856)\n",
    "\n",
    "train.count() #70,005\n",
    "test.count() #29,995\n",
    "\n",
    "#Need to cache the data to speed up training\n",
    "train.cache()\n",
    "test.cache()\n",
    "\n",
    "#Setting up the parameters for ALS\n",
    "rank = 5 # Latent Factors to be made\n",
    "numIterations = 10 # Times to repeat process\n",
    "#Create the model on the training data\n",
    "model = ALS.train(train, rank, numIterations)\n",
    "\n",
    "#Examine the latent features for one product\n",
    "model.productFeatures().first()\n",
    "#(12, array('d', [-0.29417645931243896, 1.8341970443725586, \n",
    "    #-0.4908868968486786, 0.807500958442688, -0.8945541977882385]))\n",
    "\n",
    "#Examine the latent features for one user\n",
    "model.userFeatures().first()\n",
    "#(12, array('d', [1.1348751783370972, 2.397622585296631,\n",
    "    #-0.9957215785980225, 1.062819480895996, 0.4373367130756378]))\n",
    "\n",
    "# For Product X, Find N Users to Sell To\n",
    "model.recommendUsers(242,100)\n",
    "\n",
    "# For User Y Find N Products to Promote\n",
    "model.recommendProducts(196,10)\n",
    "\n",
    "#Predict Single Product for Single User\n",
    "model.predict(196, 242)\n",
    "\n",
    "# Predict Multi Users and Multi Products\n",
    "# Pre-Processing\n",
    "pred_input = train.map(lambda x:(x[0],x[1]))   \n",
    "\n",
    "# Lots of Predictions\n",
    "#Returns Ratings(user, item, prediction)\n",
    "pred = model.predictAll(pred_input) \n",
    "\n",
    "#Get Performance Estimate\n",
    "#Organize the data to make (user, product) the key)\n",
    "true_reorg = train.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred.map(lambda x:((x[0],x[1]), x[2]))\n",
    "\n",
    "#Do the actual join\n",
    "true_pred = true_reorg.join(pred_reorg)\n",
    "\n",
    "#Need to be able to square root the Mean-Squared Error\n",
    "from math import sqrt\n",
    "\n",
    "MSE = true_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "RMSE = sqrt(MSE)#Results in 0.7629908117414474\n",
    "\n",
    "#Test Set Evaluation\n",
    "#More dense, but nothing we haven't done before\n",
    "test_input = test.map(lambda x:(x[0],x[1])) \n",
    "pred_test = model.predictAll(test_input)\n",
    "test_reorg = test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred_test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "test_pred = test_reorg.join(pred_reorg)\n",
    "test_MSE = test_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "test_RMSE = sqrt(test_MSE)#1.0145549956596238\n",
    "\n",
    "print test_MSE\n",
    "\n",
    "#If you're happy, save your model!\n",
    "model.save(sc,\"../out/ml-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
